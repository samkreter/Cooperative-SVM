\documentclass[journal]{IEEEtran}

\ifCLASSINFOpdf
\else
\fi

\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Implementing an SVM\\ A shot in the dark}

\author{Daniel Hanson, Sam Kreter, Brendan Marsh, and Christina R.S. Mosnick}

\markboth{ECE/CS 4720 (Spring 2016): Introduction to Machine Learning and Pattern Recognition}
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}

\maketitle

\begin{abstract}
The abstract goes here.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}

\IEEEPARstart{O}{ur} introduction paragraph goes here. This is just some sample text to fill up the space.

%\hfill mds

%\hfill August 26, 2015

\subsection{Subsection \#1}
Subsection text here. Sample text and stuff goes here.

\section{Implementation}
    Our SVM implementation first used a parser class to transform the input data.  The parser is able to load and write numpy arrays of the testing and label data.  To transform the label sets for training individual class dividers, the parser sets all labels that are of the current class being trained to 1, and to -1 for the rest of the classes.  Next, we created a set of kernel functions to be passed into the svm instance dynamically, depending on the particular kernel needed.  We packaged the training and prediction functionalities into instances of svm classes, which additionally store trained variables such as the support vectors and the bias.  This allowed us to train and store multiple svms.\\

    The SVM class is instantiated with the pre-processed sample (feature) vectors, their corresponding labels (1 or -1), the appropriate kernel function, and its necessary parameters (such as sigma for an RBF function).  In the training function, the support vectors are computed by finding the Gram Matrix, which stores all possible combinations of data points being passed into the kernel. We then solved the dual form found by lagrangian optimization (below) using cvxopt for our quadratic programming solver.

    \subsection{Dual Representation (from Eqn. 7.2 \cite{BishopBook})}
    \begin{equation}
    \mathbf{L} = \sum\limits_{N} a_n - \frac{1}{2} \sum\limits_{n} \sum\limits_{m} a_n a_m t_n t_m \mathbf{K}
    \end{equation}

    \subsection{Quadratic Programming Problem \cite{QuadraticCVXOPT}}
    \begin{equation}
    \min_{\mathbf{x}} \frac{1}{2}\mathbf{x}^T\mathbf{P}\mathbf{x} - \mathbf{Q}^T\mathbf{x}
    \end{equation}

    \subsection{Parameters for Quadratic Programming}
    \begin{eqnarray}
    P = \sum\limits_{n} \sum\limits_{m} t_n t_m \mathbf{K}\\
    Q = \left(\begin{IEEEeqnarraybox*}[][c]{,c/c/c,}
    -1&-1&-1\\
    -1&-1&-1\\
    -1&-1&-1%
    \end{IEEEeqnarraybox*}\right)
    \end{eqnarray}
    \begin{itemize}
    \item Where Q's dimensions are determined by number of samples
    \end{itemize}

    \subsection{Constraints}
    \begin{equation}
    \mathbf{G} \mathbf{x} \preceq \mathbf{h}
    \end{equation}

    \begin{equation}
    a_n \geq 0 \rightarrow -a_n \leq 0
    \end{equation}

    \begin{equation}
    a_n \leq \mathbf{C}
    \end{equation}

    \begin{equation}
    std\mathbf{G} \left(\begin{IEEEeqnarraybox*}[][c]{,c/c/c,}
    -1&0&0\\
    0&-1&0\\
    0&0&-1%
    \end{IEEEeqnarraybox*}\right)
    \end{equation}

    \begin{equation}
    std\mathbf{H} \left(\begin{IEEEeqnarraybox*}[][c]{,c/c/c,}
    0&0&0\\
    0&0&0\\
    0&0&0%
    \end{IEEEeqnarraybox*}\right)
    \end{equation}

    \begin{equation}
    slack\mathbf{G} \left(\begin{IEEEeqnarraybox*}[][c]{,c/c/c,}
    1&0&0\\
    0&1&0\\
    0&0&1%
    \end{IEEEeqnarraybox*}\right)
    \end{equation}

    \begin{equation}
    slack\mathbf{H} \left(\begin{IEEEeqnarraybox*}[][c]{,c/c/c,}
    \mathbf{C}&\mathbf{C}&\mathbf{C}\\
    \mathbf{C}&\mathbf{C}&\mathbf{C}\\
    \mathbf{C}&\mathbf{C}&\mathbf{C}%
    \end{IEEEeqnarraybox*}\right)
    \end{equation}

    \begin{equation}
    \mathbf{A} \mathbf{x} \leq \mathbf{b}
    \end{equation}

    \begin{equation}
    \sum\limits_{n = 1}^Na_n t_n = 0
    \end{equation}

    \begin{equation}
    \mathbf{A} = \mathbf{y} \left(\begin{IEEEeqnarraybox*}[][c]{,c/c/c,}
    1&1&1\\
    1&1&1\\
    1&1&1%
    \end{IEEEeqnarraybox*}\right)
    \end{equation}

    \begin{equation}
    \mathbf{B} = \left(\begin{IEEEeqnarraybox*}[][c]{,c/c/c,}
    0&0&0\\
    0&0&0\\
    0&0&0%
    \end{IEEEeqnarraybox*}\right)
    \end{equation}

    \subsection{Bias}
    Solving for a gives us the indices for the support vectors. These along with the passed in data we now have the vectors, labels and weights of the support vectors we can use to find the bias and start computing predictions on the functions. The bias is simply computed from the form in equation \ref{eq:bias}.

    \begin{equation}
    \label{eq:bias}
    b = \frac{1}{N_M} \sum\limits_{n \in M} (t_n - \sum\limits_{m \in S} a_m t_m k(\mathbf{x}_n, \mathbf{x}_m))
    \end{equation}

    \subsection{Prediction}
    With this we have everything we need to implement the predict function from equation \ref{eq:prediction}.

    \begin{equation}
    \label{eq:prediction}
    y(\mathbf{x}) = \sum\limits_{n = 1}^N a_n t_n k(\mathbf{x}, \mathbf{x}_n) + b
    \end{equation}

    We also use this equation in finding the bias since they are essential the same equation if b is zero for equation \ref{eq:prediction}.\\

    After calling the training function with the data set, labels and kernel function we can now call the predict function passing in a test point and will return some value between 1 and -1 which we can use to classify the point.\\

    Next we tackled the problem of classifying many classes with an SVM that only supports two class classification. To do this we use two different methods for comparison. In both cases we use a 1 vs the rest model in a bootstrapping method. We train a matrix of SVMs where the columns denote SVMs trained on different classes and the rows denote SVMs of the same class but different subsets of the training data. In the first case we take the mean of each set of SVMs for a particular class. The class with the highest overall mean will be used for the final label class. In the second case, we choose the classes with the highest results and find the variances of those results. The class with the least variance is chosen for the final label since the smaller variance shows a stronger confidence in the result.\\

    Finally we have implemented pickling function to write and load the trained SVMs to a file in order to persist the state of the trained SVMs instead of having to train the SVMs on the same training data over and over again. This can be very beneficial for using larger bootstrapping samples or having more classes where it would take a large amount of time to train.\\




\section{Experiments}

After the SVMs were all trained using the bootstrapping method, we used a committee-waterfall approach to determine the best class for each test point.  In order to do this, the SVMs are grouped by classifier, with 7 independently trained SVMs per each of the 8 classifiers.  Each test point is run through each of the 7*8=56 SVMs.  When committee results are gathered, if the point has less than 4 committee votes for each classifier, it is unclassified.  If the point has 4 or more votes from just one classifier group, it is classified to that group. If the point has 4 or more votes from multiple classification committees, it is classified to the committee with the most votes, or in the event of a tie, to a random choice between the tie.

\section{Conclusion}
Conclusion paragraph text goes here.

\section*{Acknowledgment}

Christina would like to thank her mom for her support.

\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\begin{thebibliography}{1}

\bibitem{BishopBook}
  Bishop, Christopher M. \emph{Pattern Recognition And Machine Learning.} New York: Springer, 2006. Print.

\bibitem{TullochSVMpy}
  Tulloch, Andrew. \emph{A Basic Soft-Margin Kernel SVM Implementation In Python} Tullo.ch. N.p., 2013. Web. 24 Mar. 2016.

\bibitem{QuadraticCVXOPT}
  \emph{Quadratic Programming With Python And CVXOPT. N.p., 2016. Web. 24 Mar. 2016.}

\bibitem{EffectiveNumpy}
  \emph{How To Calculate A Gaussian Kernel Effectively In Numpy.} Stats.stackexchange.com. N.p., 2016. Web. 24 Mar. 2016.

\bibitem{Pdist}
  \emph{Scipy.Spatial.Distance.Pdist â€” Scipy V0.17.0 Reference Guide.} Docs.scipy.org. N.p., 2016. Web. 24 Mar. 2016.

\end{thebibliography}

\end{document}